{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This week, I used a small dataset to test my toy deep learning + survival analysis model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adagrad\n",
    "from lifelines.utils import concordance_index\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loss function is adapted from DeepSurv: Personalized Treatment Recommender System Using A Cox Proportional Hazards Deep Neural Network.\n",
    "<cite>Katzman J L, Shaham U, Cloninger A, et al. DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network[J]. Bmc Medical Research Methodology, 2018, 18(1):24.</cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(E):\n",
    "    def loss(y_true,y_pred):\n",
    "        hazard_ratio = K.exp(y_pred)\n",
    "        log_risk = K.log(K.cumsum(hazard_ratio))\n",
    "        uncensored_likelihood = K.transpose(y_pred) - log_risk\n",
    "        censored_likelihood = uncensored_likelihood * E\n",
    "        num_observed_event = K.sum([float(e) for e in E])\n",
    "        return K.sum(censored_likelihood) / num_observed_event * (-1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple and sallow model to test whether a deep learning model can integrate with the Cox proportional-hazards model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(input_shape, size=32):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, activation='relu', kernel_initializer='glorot_uniform',\n",
    "    input_shape=input_shape))\n",
    "    model.add(Dense(size, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Dropout(0.8))\n",
    "    # model.add(Dense(128, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation=\"linear\", kernel_initializer='glorot_uniform', \n",
    "    kernel_regularizer=l2(0.01), activity_regularizer=l2(0.01)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the manual created data to the form which fits the learning network above. The data have been preprocessed by the same way as the experiment \"baseline_establishment\". Aa a result, it's from the \"dummy.xlsx\" that experiment creaeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(fp):\n",
    "    df = pd.read_excel(fp)\n",
    "    y = (df.loc[:, [\"duration\",\"observed\"]]).values\n",
    "    x = (df.loc[:,'epi_area_<25% (Low)':]).values\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_data('dummy.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760 samples are here. Each sample has 21 features\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.shape[0]} samples are here. Each sample has {x.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                704       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,793\n",
      "Trainable params: 1,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "toy_model = gen_model(x.shape[1:])\n",
    "toy_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I use  concordance index (CI) to judge the performance of the model. The concordance index (CI), which quantifies the quality of rankings, is the standard performance measure for model assessment in survival analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, x, y, e):\n",
    "    hr_pred=model.predict(x)\n",
    "    hr_pred=np.exp(hr_pred)\n",
    "    ci=concordance_index(y,-hr_pred,e)\n",
    "    return ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caveat! Usually, deep learning models will shuffle the data, that is to change the order of data randomly. In doing so, the models can perform well in different circumstances. However, for survival analysis, many data are censored, i.e. the event didn't happen in some cases. To make these censored data useful, all data should keep in a chronological order, or the model cannot be trained successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to repeat it conveniently, I combined these functions into a monolithic one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(fp, dst, size=32):\n",
    "    x, t_e = gen_data(fp)\n",
    "    x_train, x_test, t_e_train, t_e_test = train_test_split(x, t_e, test_size=0.2, \n",
    "                                                            random_state=42)\n",
    "    # print(t_e_train[:10])\n",
    "    y_train, e_train = t_e_train[:, 0], t_e_train[:, 1]\n",
    "    y_test, e_test = t_e_test[:, 0], t_e_test[:, 1]\n",
    "\n",
    "    sort_idx = np.argsort(y_train)[::-1] #! chronological order\n",
    "    x_train = x_train[sort_idx]\n",
    "    y_train = y_train[sort_idx]\n",
    "    e_train = e_train[sort_idx]\n",
    "\n",
    "    x_t_shape = np.shape(x_train)\n",
    "    print('{} training images have prepared, shape is {}\\\n",
    "    and {}'.format(len(x_train), x_t_shape, np.shape(y_train)))\n",
    "    print('{} test images have prepared, shape is {}\\\n",
    "    and {}'.format(len(x_test), np.shape(x_test), np.shape(y_test)))\n",
    "\n",
    "    model = gen_model(x_t_shape[1:], size)\n",
    "    model.summary()\n",
    "    ada = Adagrad(lr=1e-3, decay=0.1)\n",
    "    model.compile(loss=negative_log_likelihood(e_train), optimizer=ada)\n",
    "\n",
    "    cheak_list = [EarlyStopping(monitor='loss', patience=10),\n",
    "                ModelCheckpoint(filepath=os.path.join(dst, 'toy.h5')\n",
    "                , monitor='loss', save_best_only=True),\n",
    "                TensorBoard(log_dir=os.path.join(dst, 'toy_log'), \n",
    "                histogram_freq=0)]\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=len(e_train),\n",
    "        epochs=100,\n",
    "        verbose=True,\n",
    "        callbacks=cheak_list,\n",
    "        shuffle=False)\n",
    "    \n",
    "    ci = eval(model, x_train, y_train, e_train)\n",
    "    ci_val = eval(model, x_test, y_test, e_test)\n",
    "    \n",
    "    with open(os.path.join(dst, 'toy_outcome.txt'), 'w+') as out:\n",
    "        line = 'Concordance Index for training dataset:{},\\n\\\n",
    "                Concordance Index for test dataset:{}'.format(ci, ci_val)\n",
    "        print(line)\n",
    "        out.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608 training images have prepared, shape is (608, 21)    and (608,)\n",
      "152 test images have prepared, shape is (152, 21)    and (152,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 32)                704       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,793\n",
      "Trainable params: 1,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "Epoch 1/100\n",
      "608/608 [==============================] - 1s 972us/step - loss: 3430.7344\n",
      "Epoch 2/100\n",
      "608/608 [==============================] - 0s 33us/step - loss: 3383.2307\n",
      "Epoch 3/100\n",
      "608/608 [==============================] - 0s 44us/step - loss: 3441.7512\n",
      "Epoch 4/100\n",
      "608/608 [==============================] - 0s 87us/step - loss: 3469.1018\n",
      "Epoch 5/100\n",
      "608/608 [==============================] - 0s 67us/step - loss: 3411.0161\n",
      "Epoch 6/100\n",
      "608/608 [==============================] - 0s 107us/step - loss: 3472.9944\n",
      "Epoch 7/100\n",
      "608/608 [==============================] - 0s 71us/step - loss: 3499.4111\n",
      "Epoch 8/100\n",
      "608/608 [==============================] - 0s 72us/step - loss: 3390.4932\n",
      "Epoch 9/100\n",
      "608/608 [==============================] - 0s 74us/step - loss: 3437.4207\n",
      "Epoch 10/100\n",
      "608/608 [==============================] - 0s 89us/step - loss: 3403.0247\n",
      "Epoch 11/100\n",
      "608/608 [==============================] - 0s 89us/step - loss: 3392.2012\n",
      "Epoch 12/100\n",
      "608/608 [==============================] - 0s 110us/step - loss: 3448.3027\n",
      "Concordance Index for training dataset:0.49965677020765403,\n",
      "                Concordance Index for test dataset:0.4572463768115942\n"
     ]
    }
   ],
   "source": [
    "fp = 'dummy.xlsx'\n",
    "dst = 'toy_experiment'\n",
    "os.makedirs(dst, exist_ok=True)\n",
    "run_model(fp, dst, size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can run correctly. However, Concordance Indexes for training dataset and for test dataset are both pretty low. It's possibly due to the limited capacity of the model. So I tried to increace its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608 training images have prepared, shape is (608, 21)    and (608,)\n",
      "152 test images have prepared, shape is (152, 21)    and (152,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 256)               5632      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 71,681\n",
      "Trainable params: 71,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "608/608 [==============================] - 1s 1ms/step - loss: 3290.6594\n",
      "Epoch 2/100\n",
      "608/608 [==============================] - 0s 51us/step - loss: 3302.4729\n",
      "Epoch 3/100\n",
      "608/608 [==============================] - 0s 250us/step - loss: 3280.5012\n",
      "Epoch 4/100\n",
      "608/608 [==============================] - 0s 119us/step - loss: 3295.6787\n",
      "Epoch 5/100\n",
      "608/608 [==============================] - 0s 197us/step - loss: 3259.3044\n",
      "Epoch 6/100\n",
      "608/608 [==============================] - 0s 115us/step - loss: 3288.4553\n",
      "Epoch 7/100\n",
      "608/608 [==============================] - 0s 104us/step - loss: 3285.8350\n",
      "Epoch 8/100\n",
      "608/608 [==============================] - 0s 141us/step - loss: 3262.2722\n",
      "Epoch 9/100\n",
      "608/608 [==============================] - 0s 99us/step - loss: 3269.8975\n",
      "Epoch 10/100\n",
      "608/608 [==============================] - 0s 123us/step - loss: 3304.2815\n",
      "Epoch 11/100\n",
      "608/608 [==============================] - 0s 104us/step - loss: 3264.8330\n",
      "Epoch 12/100\n",
      "608/608 [==============================] - 0s 100us/step - loss: 3287.3108\n",
      "Epoch 13/100\n",
      "608/608 [==============================] - 0s 112us/step - loss: 3263.6111\n",
      "Epoch 14/100\n",
      "608/608 [==============================] - 0s 125us/step - loss: 3246.3062\n",
      "Epoch 15/100\n",
      "608/608 [==============================] - 0s 120us/step - loss: 3255.2888\n",
      "Epoch 16/100\n",
      "608/608 [==============================] - 0s 130us/step - loss: 3289.6387\n",
      "Epoch 17/100\n",
      "608/608 [==============================] - 0s 132us/step - loss: 3248.7759\n",
      "Epoch 18/100\n",
      "608/608 [==============================] - 0s 141us/step - loss: 3274.4080\n",
      "Epoch 19/100\n",
      "608/608 [==============================] - 0s 94us/step - loss: 3259.2812\n",
      "Epoch 20/100\n",
      "608/608 [==============================] - 0s 104us/step - loss: 3255.9429\n",
      "Epoch 21/100\n",
      "608/608 [==============================] - 0s 128us/step - loss: 3262.1006\n",
      "Epoch 22/100\n",
      "608/608 [==============================] - 0s 112us/step - loss: 3266.5876\n",
      "Epoch 23/100\n",
      "608/608 [==============================] - 0s 135us/step - loss: 3265.5132\n",
      "Epoch 24/100\n",
      "608/608 [==============================] - 0s 155us/step - loss: 3266.4768\n",
      "Concordance Index for training dataset:0.7767719238029861,\n",
      "                Concordance Index for test dataset:0.5398550724637681\n"
     ]
    }
   ],
   "source": [
    "run_model(fp, dst, size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the outcome become better now. Still the Concordance Index for test dataset is low. That implies the capacity of the model is too powerful to generalize varing data. In this case, we should shrink it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608 training images have prepared, shape is (608, 21)    and (608,)\n",
      "152 test images have prepared, shape is (152, 21)    and (152,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               2816      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 19,457\n",
      "Trainable params: 19,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "608/608 [==============================] - 1s 1ms/step - loss: 3367.4153\n",
      "Epoch 2/100\n",
      "608/608 [==============================] - 0s 41us/step - loss: 3353.6958\n",
      "Epoch 3/100\n",
      "608/608 [==============================] - 0s 58us/step - loss: 3352.1772\n",
      "Epoch 4/100\n",
      "608/608 [==============================] - 0s 92us/step - loss: 3322.1770\n",
      "Epoch 5/100\n",
      "608/608 [==============================] - 0s 128us/step - loss: 3297.5479\n",
      "Epoch 6/100\n",
      "608/608 [==============================] - 0s 104us/step - loss: 3299.7810\n",
      "Epoch 7/100\n",
      "608/608 [==============================] - 0s 90us/step - loss: 3309.1680\n",
      "Epoch 8/100\n",
      "608/608 [==============================] - 0s 138us/step - loss: 3332.6572\n",
      "Epoch 9/100\n",
      "608/608 [==============================] - 0s 236us/step - loss: 3295.2800\n",
      "Epoch 10/100\n",
      "608/608 [==============================] - 0s 123us/step - loss: 3330.1746\n",
      "Epoch 11/100\n",
      "608/608 [==============================] - 0s 92us/step - loss: 3286.2173\n",
      "Epoch 12/100\n",
      "608/608 [==============================] - 0s 105us/step - loss: 3323.2944\n",
      "Epoch 13/100\n",
      "608/608 [==============================] - 0s 95us/step - loss: 3294.8164\n",
      "Epoch 14/100\n",
      "608/608 [==============================] - 0s 133us/step - loss: 3291.9958\n",
      "Epoch 15/100\n",
      "608/608 [==============================] - 0s 92us/step - loss: 3327.3792\n",
      "Epoch 16/100\n",
      "608/608 [==============================] - 0s 123us/step - loss: 3312.3618\n",
      "Epoch 17/100\n",
      "608/608 [==============================] - 0s 110us/step - loss: 3277.4421\n",
      "Epoch 18/100\n",
      "608/608 [==============================] - 0s 82us/step - loss: 3289.2654\n",
      "Epoch 19/100\n",
      "608/608 [==============================] - 0s 79us/step - loss: 3300.8882\n",
      "Epoch 20/100\n",
      "608/608 [==============================] - 0s 125us/step - loss: 3310.5708\n",
      "Epoch 21/100\n",
      "608/608 [==============================] - 0s 100us/step - loss: 3273.1304\n",
      "Epoch 22/100\n",
      "608/608 [==============================] - 0s 102us/step - loss: 3292.5657\n",
      "Epoch 23/100\n",
      "608/608 [==============================] - 0s 58us/step - loss: 3338.6702\n",
      "Epoch 24/100\n",
      "608/608 [==============================] - 0s 63us/step - loss: 3317.9348\n",
      "Epoch 25/100\n",
      "608/608 [==============================] - 0s 86us/step - loss: 3293.8967\n",
      "Epoch 26/100\n",
      "608/608 [==============================] - 0s 82us/step - loss: 3295.5540\n",
      "Epoch 27/100\n",
      "608/608 [==============================] - 0s 100us/step - loss: 3307.9514\n",
      "Epoch 28/100\n",
      "608/608 [==============================] - 0s 102us/step - loss: 3309.7830\n",
      "Epoch 29/100\n",
      "608/608 [==============================] - 0s 143us/step - loss: 3327.2607\n",
      "Epoch 30/100\n",
      "608/608 [==============================] - 0s 289us/step - loss: 3319.9963\n",
      "Epoch 31/100\n",
      "608/608 [==============================] - 0s 112us/step - loss: 3274.9617\n",
      "Concordance Index for training dataset:0.7166638064183971,\n",
      "                Concordance Index for test dataset:0.5594202898550724\n"
     ]
    }
   ],
   "source": [
    "run_model(fp, dst, size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the population of samples is small, even though the model could learn from the data, and the outcome followed the basic rules of deep learning model, the result was not desired. I should do more research nextweek."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
